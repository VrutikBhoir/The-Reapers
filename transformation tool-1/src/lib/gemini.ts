
import { GoogleGenerativeAI } from "@google/generative-ai";
import type { 
  GeminiInferredConstraint, 
  GeminiValidationResult, 
  DataTransformationAudit,
  StructuredDataQualityAssessment,
  GeminiAnalysisResult
} from '../types';

// Access API key safely
const API_KEY = import.meta.env.VITE_GEMINI_API_KEY || '';

// Initialize Gemini
// We'll handle the case where API_KEY is missing by returning a mock/empty result or throwing
const genAI = new GoogleGenerativeAI(API_KEY);
const model = genAI.getGenerativeModel({ 
  model: "gemini-2.5-flash-lite",
  generationConfig: { responseMimeType: "application/json" }
});

/**
 * Part 1: AI-Driven Semantic Validation
 * Infers validation rules based on column names and sample data.
 */
export async function inferSemanticValidationRules(
  columns: string[], 
  sampleData: any[]
): Promise<GeminiInferredConstraint[]> {
  if (!API_KEY) return [];

  // Prepare a sample of data (e.g., first 20 rows) to send to Gemini
  const samples = sampleData.slice(0, 20).map(row => {
    const subset: Record<string, any> = {};
    columns.forEach(col => subset[col] = row[col]);
    return subset;
  });

  const prompt = `
    Analyze the following dataset columns and sample values. 
    For each column, infer the real-world meaning, expected data type, and validation constraints.
    
    Columns: ${JSON.stringify(columns)}
    Sample Data: ${JSON.stringify(samples)}

    Return a JSON array where each object has:
    - field: column name
    - inferred_intent: e.g., "human_age", "email", "product_price"
    - expected_type: "integer", "float", "string", "date", "boolean"
    - constraints: { min, max, pattern (regex string), options (array of strings), custom_rule (description) }
    - confidence: 0.0 to 1.0

    Example:
    [
      { 
        "field": "age", 
        "inferred_intent": "human_age", 
        "expected_type": "integer", 
        "constraints": { "min": 0, "max": 120 }, 
        "confidence": 0.95 
      }
    ]
  `;

  try {
    const result = await model.generateContent(prompt);
    const text = result.response.text();
    return JSON.parse(text) as GeminiInferredConstraint[];
  } catch (error) {
    console.error("Gemini inference error:", error);
    return [];
  }
}

/**
 * Part 2: AI-Enhanced Validation Execution
 * Applies the inferred rules to the dataset.
 * Note: This runs locally for performance, using the rules generated by Gemini.
 */
export function applyGeminiValidation(
  data: any[], 
  rules: GeminiInferredConstraint[]
): GeminiValidationResult[] {
  const results: GeminiValidationResult[] = [];

  data.forEach((row, rowIndex) => {
    rules.forEach(rule => {
      const val = row[rule.field];
      if (val === null || val === undefined || val === '') return; // Skip empty for now, or handle based on required?

      let isValid = true;
      let reason = '';

      // Type check
      if (rule.expected_type === 'integer' && !Number.isInteger(Number(val))) {
        isValid = false;
        reason = `Expected integer, got ${typeof val}`;
      } else if ((rule.expected_type === 'float' || rule.expected_type === 'numeric') && isNaN(Number(val))) {
        isValid = false;
        reason = `Expected number, got ${val}`;
      }

      // Range check
      if (isValid && rule.constraints.min !== undefined && Number(val) < rule.constraints.min) {
        isValid = false;
        reason = `Value ${val} is below minimum ${rule.constraints.min}`;
      }
      if (isValid && rule.constraints.max !== undefined && Number(val) > rule.constraints.max) {
        isValid = false;
        reason = `Value ${val} exceeds maximum ${rule.constraints.max}`;
      }

      // Pattern check
      if (isValid && rule.constraints.pattern) {
        try {
          const regex = new RegExp(rule.constraints.pattern);
          if (!regex.test(String(val))) {
            isValid = false;
            reason = `Value does not match pattern ${rule.constraints.pattern}`;
          }
        } catch (e) {
          // Ignore invalid regex from AI
        }
      }

      // Options check
      if (isValid && rule.constraints.options && rule.constraints.options.length > 0) {
         if (!rule.constraints.options.includes(String(val))) {
           isValid = false;
           reason = `Value not in allowed options: ${rule.constraints.options.join(', ')}`;
         }
      }

      if (!isValid) {
        results.push({
          row_index: rowIndex,
          field: rule.field,
          value: val,
          status: 'warning', // Default to warning as these are "inferred" rules
          reason: `${rule.inferred_intent} violation: ${reason}`
        });
      }
    });
  });

  return results;
}

/**
 * Part 3: Structured Data Correctness Check
 * Evaluates the quality of the cleaned dataset.
 * Uses a hybrid of local metrics and Gemini assessment.
 */
export async function assessStructuredQuality(
  cleanedData: any[]
): Promise<StructuredDataQualityAssessment> {
  if (!API_KEY) return { score: 0, improvements: [], remaining_risks: [], consistency_check: { status: 'warn', details: 'AI unavailable' } };

  // Calculate some basic stats locally
  const totalCells = cleanedData.length * Object.keys(cleanedData[0] || {}).length;
  const nullCount = cleanedData.reduce((acc, row) => acc + Object.values(row).filter(v => v === null || v === '').length, 0);
  const nullPercentage = totalCells > 0 ? (nullCount / totalCells) : 0;

  // Send sample to Gemini for semantic consistency check
  const sample = cleanedData.slice(0, 10);
  
  const prompt = `
    Evaluate the quality of this structured dataset sample.
    
    Sample: ${JSON.stringify(sample)}
    Null Percentage: ${(nullPercentage * 100).toFixed(2)}%

    Assess:
    1. Logical consistency (do fields make sense together?)
    2. Semantic improvement (are values normalized?)
    
    Return JSON:
    {
      "score": number (0-100),
      "improvements": string[],
      "remaining_risks": string[],
      "consistency_check": {
        "status": "pass" | "fail" | "warn",
        "details": "explanation"
      }
    }
  `;

  try {
    const result = await model.generateContent(prompt);
    return JSON.parse(result.response.text()) as StructuredDataQualityAssessment;
  } catch (error) {
    console.error("Gemini quality assessment error:", error);
    return {
      score: 0,
      improvements: [],
      remaining_risks: ["Failed to assess quality via AI"],
      consistency_check: { status: 'warn', details: 'AI assessment failed' }
    };
  }
}

/**
 * Part 4: Data Transformation Audit
 * Compares raw and cleaned data to produce metrics.
 * Implemented locally for precision.
 */
export function auditDataTransformation(
  raw: any[], 
  cleaned: any[]
): DataTransformationAudit {
  // Assuming 1:1 row mapping for simplicity, or we need an ID. 
  // If rows were deleted, raw.length > cleaned.length.
  
  const audit: DataTransformationAudit = {
    rows_total: raw.length,
    rows_altered: 0,
    rows_unchanged: 0,
    rows_deleted: raw.length - cleaned.length, // simplistic assumption
    values_corrected: 0,
    values_normalized: 0,
    nulls_filled: 0,
    anomalies_flagged: 0 // This needs external input or diffing logic
  };

  // We iterate up to the smaller length to compare
  const limit = Math.min(raw.length, cleaned.length);
  
  for (let i = 0; i < limit; i++) {
    const r = raw[i];
    const c = cleaned[i];
    
    // A better approach if we don't have the map here:
    // Just count how many rows are not deep-equal.
    if (JSON.stringify(r) !== JSON.stringify(c)) {
      audit.rows_altered++;
    } else {
      audit.rows_unchanged++;
    }

    // Value level stats (approximate without exact column map)
    // We can check for nulls filled
    Object.keys(c).forEach(key => {
        // If raw had this key
        if (key in r) {
            if ((r[key] === null || r[key] === '') && (c[key] !== null && c[key] !== '')) {
                audit.nulls_filled++;
            }
            if (r[key] !== c[key]) {
                // If it wasn't null but changed, count as correction/normalization
                 // simplistic heuristic
                 audit.values_corrected++;
            }
        }
    });
  }
  
  return audit;
}

/**
 * Part 5: Human-Readable AI Insights
 * Generates a summary based on the audit and quality assessment.
 */
export async function generateAiInsights(
  audit: DataTransformationAudit,
  quality: StructuredDataQualityAssessment,
  validationSummary: { ruleCount: number, violationCount: number }
): Promise<string> {
  if (!API_KEY) return "AI Insights unavailable (Missing API Key).";

  const prompt = `
    Generate a natural language summary of the data transformation process.
    
    Audit Metrics: ${JSON.stringify(audit)}
    Quality Assessment: ${JSON.stringify(quality)}
    Validation: ${JSON.stringify(validationSummary)}

    Explain:
    - What changed
    - Why it changed
    - How data quality improved
    - What risks remain
    
    Keep it concise and professional.
  `;

  try {
    const modelText = genAI.getGenerativeModel({ model: "gemini-2.5-flash-lite" }); // Default text output
    const result = await modelText.generateContent(prompt);
    return result.response.text();
  } catch (error) {
    return "Failed to generate insights.";
  }
}

/**
 * Main Orchestrator
 */
export async function runGeminiAnalysis(
  raw: any[],
  cleaned: any[],
  columns: string[]
): Promise<GeminiAnalysisResult> {
  // 1. Infer Rules
  const rules = await inferSemanticValidationRules(columns, raw);
  
  // 2. Validate (using inferred rules on CLEANED data? or RAW? Prompt says "Apply... to actual column values". Usually we validate cleaned data.)
  // But Part 1 says "Input... Original raw dataset... Cleaned dataset". 
  // Let's validate the CLEANED data to show remaining issues, or validate RAW to show what was fixed?
  // "Gemini must evaluate whether the cleaned dataset is..." (Part 3).
  // Part 2 "Apply Gemini-inferred constraints... Detect violations". 
  // Usually you want to know if the final output is valid.
  const validationResults = applyGeminiValidation(cleaned, rules);

  // 3. Audit
  const audit = auditDataTransformation(raw, cleaned);
  audit.anomalies_flagged = validationResults.length;

  // 4. Quality
  const quality = await assessStructuredQuality(cleaned);

  // 5. Insights
  const summary = await generateAiInsights(audit, quality, { 
    ruleCount: rules.length, 
    violationCount: validationResults.length 
  });

  return {
    ai_inferred_validation_rules: rules,
    ai_validation_results: validationResults,
    data_transformation_audit: audit,
    structured_data_quality_assessment: quality,
    natural_language_summary: summary
  };
}
